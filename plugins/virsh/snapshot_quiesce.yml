- name: Ensure that time is right everywhere
  hosts: "undercloud:overcloud_nodes"
  gather_facts: false
  any_errors_fatal: true
  tasks:
    - name: Populate service facts
      service_facts:

    - name: Reset the time if ntpd service
      become: true
      when:
        - "'ntpd.service' in ansible_facts.services"
        - ansible_facts.services['ntpd.service'].state == "running"
      block:
      - name: Stop the time service
        service:
          name: ntpd
          state: stopped
      - name: Reset the time
        shell: >-
          ntpd -q -g -x
        changed_when: false
      - name: Start the time service
        service:
          name: ntpd
          state: started

    - name: Reset the time if chronyd
      become: true
      when:
        - "'chronyd.service' in ansible_facts.services"
        - ansible_facts.services['chronyd.service'].state == "running"
      block:
      - name: Reset the time
        shell: >-
          chronyc makestep
        changed_when: false

- name: Ensure that the Ceph cluster is up and healthy
  hosts: "{{ ('monitors' in groups) | ternary('monitors', 'controller') }}"
  gather_facts: false
  any_errors_fatal: true
  tasks:
    - name: Only run this on one of the hosts
      run_once: true
      when: "'ceph' in groups"
      block:
      - name: Get a name of Ceph cluster
        become: true
        shell: |
          set -o pipefail
          ps -p $(pidof ceph-mon) -o args --no-headers | grep -o 'cluster.*' | cut -d' ' -f2
        changed_when: false
        register: ceph_cluster_name

      - name: Identify the container runtime (if any)
        shell: |-
          runtime=$(command -v podman || command -v docker)
          if [[ $runtime != '' ]]; then
            basename $runtime
          else
            echo none
          fi
        register: _container_runtime
        changed_when: false

      - name: Set the ceph command prefix
        set_fact:
          _ceph_cmd_prefix: >-
            {% if _container_runtime.stdout != "none" %}
            {{ _container_runtime.stdout }}
            exec
            ceph-mon-{{ inventory_hostname }}
            {% endif %}
            ceph
            -c
            /etc/ceph/{{ ceph_cluster_name.stdout }}.conf

      - name: Wait for OSDs to come back
        become: true
        command: >-
          {{ _ceph_cmd_prefix }} pg stat
        changed_when: false
        register: active_osd
        until: active_osd.stdout.find("active+clean") > -1
        retries: 5
        delay: 60

      - name: Wait for cluster health to be OK
        become: true
        command: >-
          {{ _ceph_cmd_prefix }} status
        changed_when: false
        register: _ceph_health
        until: _ceph_health.stdout.find("HEALTH_OK") > -1
        retries: 5
        delay: 60

- name: Ensure that the Pacemaker cluster and all its resources are up
  hosts: controller
  gather_facts: false
  any_errors_fatal: true
  tasks:
    - block:
        - name: Wait for core cluster hosts to come online
          shell: |
            set -o pipefail
            crm_mon -1 | grep -w 'Online:' | grep {{ inventory_hostname }}
          changed_when: false
          register: _pcs_status
          retries: 5
          delay: 60
          until: _pcs_status is not failed

        - name: Wait for the resources to come online
          shell: |
            crm_mon -1
          changed_when: false
          register: _bundle_status
          retries: 5
          delay: 60
          until:
            - not(_bundle_status.stdout is search("Stopped.*" ~ inventory_hostname))
            - not(_bundle_status.stdout is search("Starting.*" ~ inventory_hostname))

        - name: Wait for cluster to be OK
          shell: |
            crm_mon -s
          changed_when: false
          register: _pcs_health
          retries: 5
          delay: 60
          until: _pcs_health.stdout.find("CLUSTER OK") > -1
          run_once: true
      become: true

- name: Ensure that all the core OpenStack services/agents are up
  hosts: undercloud
  gather_facts: false
  any_errors_fatal: true
  vars:
    overcloud_rc: "/home/stack/{{ _overcloud_stack_name.stdout }}rc"
  tasks:
    - name: Discover the overcloud stack name
      shell: |
        source /home/stack/stackrc
        openstack stack list -c 'Stack Name' -f value
      register: _overcloud_stack_name
      changed_when: false

    - name: Wait until all network agents are up
      shell: |
        source {{ overcloud_rc }}
        openstack network agent list -f json | jq '.[] | select(.State==true and .Alive==false)'
      changed_when: false
      register: _network_agents_state
      retries: 15
      delay: 60
      until: _network_agents_state.stdout == ''

    - name: Wait until all compute services are up
      shell: |
        source {{ overcloud_rc }}
        openstack compute service list -f json | jq '.[] | select(.Status=="enabled" and .State=="down")'
      changed_when: false
      register: _compute_service_state
      retries: 15
      delay: 60
      until: _compute_service_state.stdout == ''

    - name: Wait until all volume services are up
      shell: |
        source {{ overcloud_rc }}
        openstack volume service list -f json | jq '.[] | select(.Status=="enabled" and .State=="down" and .Binary!="cinder-backup")'
      changed_when: false
      register: _volume_service_state
      retries: 15
      delay: 60
      until: _volume_service_state.stdout == ''

    # ref:
    #   https://bugzilla.redhat.com/show_bug.cgi?id=1665191
    #   https://bugzilla.redhat.com/show_bug.cgi?id=1666804
    - name: Verify that at least one cinder-backup service is active
      shell: |
        source {{ overcloud_rc }}
        openstack volume service list -f json | jq '.[] | select(.Binary=="cinder-backup" and .Status=="enabled" and .State=="up").Host' | wc -l
      changed_when: false
      register: _volume_backup_state
      retries: 15
      delay: 60
      until: _volume_backup_state.stdout == '1'
