---
- name: Import snapshots of virtual machines
  hosts: "{{ (_run_condition | bool) | ternary('hypervisor', 'none') }}"
  gather_facts: yes
  any_errors_fatal: true
  vars:
    vm_prefix: "{{ provision.prefix | default('') }}"
    src_workspace: "{{ lookup('env', 'WORKSPACE') | default('/tmp', true) }}"
    ir_cmd: "{{ (lookup('env', 'VIRTUAL_ENV') != '') | ternary(lookup('env', 'VIRTUAL_ENV') ~ '/bin/infrared', 'infrared') }}"
    ir_home: "{{ lookup('env', 'IR_HOME') | default(lookup('env', 'HOME') ~ '/.infrared', true) }}"
    inventory_cmd: "{{ (lookup('env', 'VIRTUAL_ENV') != '') | ternary(lookup('env', 'VIRTUAL_ENV') ~ '/bin/ansible-inventory', 'ansible-inventory') }}"
  tasks:
    - name: Assert that we are using a compatible hypervisor
      assert:
        that:
          - ansible_facts['os_family'] == 'RedHat'
          - ansible_facts['distribution_version'] is version('8.2', '>=')

    - name: Install required packages
      package:
        name: libguestfs-tools-c
        state: present

    - name: Get info about existing VM's
      virt:
        command: list_vms
      register: _virt_list

    - name: Create the list of existing VM's to remove
      set_fact:
        _vm_to_remove_list: "{{ _virt_list.list_vms | select('match', vm_prefix ~ provision.virsh.snapshot.servers) | list }}"

    - name: Stop any running VM's
      shell: |
        set -eo pipefail
        RETURN_CODE=0
        if ! virsh domstate {{ item }} | grep -qw 'shut off'; then
          virsh destroy {{ item }}
          RETURN_CODE=2
        fi
        exit ${RETURN_CODE}
      args:
        executable: /bin/bash
      loop: "{{ _vm_to_remove_list }}"
      register: _vm_stop
      changed_when: _vm_stop.rc == 2
      failed_when: _vm_stop.rc not in [0, 2]

    - name: Wait for all VM's to be stopped
      command: |
        virsh domstate {{ item }}
      changed_when: False
      register: _vm_shutdown
      until: _vm_shutdown.stdout.find('shut off') != -1
      retries: 5
      delay: 60
      loop: "{{ _vm_to_remove_list }}"

    - name: Delete any disk images related to running VM's
      shell: |
        set -o pipefail
        for vdisk in $(virsh domblklist {{ item }} | awk '/{{ item }}/ {print $2}'); do
          rm -f ${vdisk}
        done
      args:
        executable: /bin/bash
      loop: "{{ _vm_to_remove_list }}"

    - name: Undefine all running VM's
      virt:
        name: "{{ item }}"
        command: undefine
      failed_when: false
      loop: "{{ _vm_to_remove_list }}"

    - name: Read the exported manifest file
      slurp:
        path: "{{ provision.virsh.snapshot.path }}/manifest.json"
      register: _virsh_snapshot_manifest_file

    - name: Register manifest content as a fact
      set_fact:
        _virsh_snapshot_manifest_content: "{{ _virsh_snapshot_manifest_file.content | b64decode | from_json }}"

    - name: Import the downloaded content
      block:
        - name: Import the disk images
          copy:
            src: "{{ provision.virsh.snapshot.path }}/{{ item.path | basename }}"
            dest: "{{ provision.disk.pool }}/"
            checksum: "{{ item.checksum }}"
            remote_src: yes
          loop: "{{ _virsh_snapshot_manifest_content.files | selectattr('path', 'match', '.*qcow2$') | list }}"
          loop_control:
            label: "{{ item.path | basename }}"

        - name: Import the nvram images
          copy:
            src: "{{ provision.virsh.snapshot.path }}/{{ item.path | basename }}"
            dest: "{{ provision.disk.pool }}/../qemu/nvram/"
            checksum: "{{ item.checksum }}"
            remote_src: yes
          loop: "{{ _virsh_snapshot_manifest_content.files | selectattr('path', 'match', '.*fd$') | list }}"
          loop_control:
            label: "{{ item.path | basename }}"

        - name: Import the VM configuration
          copy:
            src: "{{ provision.virsh.snapshot.path }}/{{ item.path | basename }}"
            dest: "/etc/libvirt/qemu/"
            checksum: "{{ item.checksum }}"
            remote_src: yes
          loop: "{{ _virsh_snapshot_manifest_content.files | selectattr('path', 'match', '.*xml$') | list }}"
          loop_control:
            label: "{{ item.path | basename }}"

        - name: Fetch the infrared workspace
          fetch:
            src: "{{ provision.virsh.snapshot.path }}/infrared-workspace.tgz"
            dest: "{{ src_workspace }}/"
            flat: yes

        - name: Import the infrared workspace
          shell: |-
            IR_ACTIVE_WORKSPACE=$({{ ir_cmd }} workspace list --active)
            IR_NEW_WORKSPACE="workspace_$(date +%Y-%m-%d_%H-%M-%S)"
            {{ ir_cmd }} workspace import {{ src_workspace }}/infrared-workspace.tgz --name ${IR_NEW_WORKSPACE}
          delegate_to: localhost
          args:
            executable: /bin/bash
      always:
        - name: Clean up the snapshot path
          file:
            path: "{{ provision.virsh.snapshot.path }}"
            state: absent
          when:
            - provision.virsh.snapshot.cleanup | bool

    - name: Change machine type to work on the current hypervisor (pc)
      lineinfile:
        path: "/etc/libvirt/qemu/{{ item.path | basename }}"
        regexp: "(.*)machine='pc-i440fx.*'(.*)"
        line: "\\1machine='pc'\\2"
        backrefs: true
      loop: "{{ _virsh_snapshot_manifest_content.files | selectattr('path', 'match', '.*xml$') | list }}"
      loop_control:
        label: "{{ item.path | basename }}"

    - name: Change machine type to work on the current hypervisor (q35)
      lineinfile:
        path: "/etc/libvirt/qemu/{{ item.path | basename }}"
        regexp: "(.*)machine='pc-q35.*'(.*)"
        line: "\\1machine='q35'\\2"
        backrefs: true
      loop: "{{ _virsh_snapshot_manifest_content.files | selectattr('path', 'match', '.*xml$') | list }}"
      loop_control:
        label: "{{ item.path | basename }}"

    - name: Change pool path to the correct location
      xml:
        path: "/etc/libvirt/qemu/{{ item.server }}.xml"
        xpath: "/domain/devices/disk[@type='file']/source[@file='{{ _virsh_snapshot_manifest_content.virt_pool_path }}/{{ item.image }}']"
        attribute: "file"
        value: "{{ provision.disk.pool }}/{{ item.image }}"
      loop: "{{ server_image_map }}"
      vars:
        server_image_map: >-
          {%- set _server_image_map = [] %}
          {%- for server in _virsh_snapshot_manifest_content.servers | map(attribute='name') | list %}
          {%-   for image in _virsh_snapshot_manifest_content.files | selectattr('path', 'match', '.*' ~ server ~ '.*(qcow2|fd)$') | map(attribute='path') | map('basename') | list %}
          {%-     set _ = _server_image_map.append({'server': server, 'image': image}) %}
          {%-   endfor %}
          {%- endfor %}
          {{- _server_image_map }}

    - name: Eliminate serial if serial_files is disabled
      xml:
        path: "/etc/libvirt/qemu/{{ item }}.xml"
        xpath: "/domain/devices/serial[@type='file']"
        state: absent
      when:
        - not provision.serial.files | bool
      loop: "{{ _virsh_snapshot_manifest_content.servers | map(attribute='name') | list }}"

    - name: Eliminate console if serial_files is disabled
      xml:
        path: "/etc/libvirt/qemu/{{ item }}.xml"
        xpath: "/domain/devices/console[@type='file']"
        state: absent
      when:
        - not provision.serial.files | bool
      loop: "{{ _virsh_snapshot_manifest_content.servers | map(attribute='name') | list }}"

    - name: Change nvram path to the correct location
      lineinfile:
        path: "/etc/libvirt/qemu/{{ item.path | basename }}"
        regexp: "(.*)/var/lib/libvirt/qemu/nvram(.*)"
        line: "\\1{{ provision.disk.pool }}/../qemu/nvram/\\2"
        backrefs: true
      loop: "{{ _virsh_snapshot_manifest_content.files | selectattr('path', 'match', '.*xml$') | list }}"
      loop_control:
        label: "{{ item.path | basename }}"

    - name: Change bochs video driver for qxl
      lineinfile:
        path: "/etc/libvirt/qemu/{{ item.path | basename }}"
        regexp: "(.*)<model type=\"bochs\"(.*)"
        line: "\\1<model type=\"qxl\" ram=\"65536\"\\2"
        backrefs: true
      loop: "{{ _virsh_snapshot_manifest_content.files | selectattr('path', 'match', '.*xml$') | list }}"
      loop_control:
        label: "{{ item.path | basename }}"

    # We use command here, because the virt module needs the raw XML
    # and a lookup will not work remotely. We could slurp all the files,
    # but that's a lot of work for very little gain.
    - name: Define the VM's
      command: >-
        virsh define /etc/libvirt/qemu/{{ item.path | basename }}
      loop: "{{ _virsh_snapshot_manifest_content.files | selectattr('path', 'match', '.*xml$') | list }}"
      loop_control:
        label: "{{ item.path | basename }}"

    - name: Start any VM's that were in a running state
      virt:
        name: "{{ item.name }}"
        command: start
        state: running
      loop: "{{ _virsh_snapshot_manifest_content.servers | selectattr('state', 'equalto', 'running') | list }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Discover the list of workspace public ssh keys to be imported
      find:
        path: "{{ ir_home }}/.workspaces/active"
        patterns: "*.pub"
        file_type: file
      register: _ir_workspace_pubkeys
      delegate_to: localhost

    - name: Add the imported workspace ssh key to the remote hypervisor
      authorized_key:
        user: root
        state: present
        key: "{{ lookup('file', item.path) }}"
      loop: "{{ _ir_workspace_pubkeys.files }}"
      loop_control:
        label: "{{ item.path }}"

    - name: Discover the list of imported workspace files which need to be adjusted
      find:
        path: "{{ ir_home }}/.workspaces/active"
        pattern: "hosts*"
        file_type: file
      register: _ir_workspace_files
      delegate_to: localhost

    - name: Use the current hypervisor ssh key for direct connections to VMs
      replace:
        path: "{{ item.path }}"
        regexp: "ansible_ssh_private_key_file=\\/[a-zA-Z\\.\\/0-9_-]+\\/id_(rsa|ecdsa)[a-zA-Z0-9_-]* "
        replace: "ansible_ssh_private_key_file={{ provision.host.key }} "
      loop: "{{ _ir_workspace_files.files }}"
      loop_control:
        label: "{{ item.path }}"
      delegate_to: localhost

    - name: Use the current hypervisor key for proxy connections to VMs
      replace:
        path: "{{ item.path }}"
        regexp: '-i [\.a-zA-Z0-9\/\_-]+ '
        replace: "-i {{ provision.host.key }} "
      loop: "{{ _ir_workspace_files.files }}"
      loop_control:
        label: "{{ item.path }}"
      delegate_to: localhost

    - name: Final adjustments of the imported workspace inventory files for the new hypervisor
      replace:
        path: "{{ item.path }}"
        regexp: "{{ _virsh_snapshot_manifest_content.hypervisor_fqdn | regex_escape() }}"
        replace: "{{ provision.host.address[0] }}"
      loop: "{{ _ir_workspace_files.files }}"
      loop_control:
        label: "{{ item.path }}"
      delegate_to: localhost

    - name: Read the new inventory
      shell: >-
        {{ inventory_cmd }} -i {{ ir_home }}/.workspaces/active/hosts --list
      register: _new_ir_inventory
      delegate_to: localhost
      args:
        executable: /bin/bash
      environment:
        ANSIBLE_VERBOSITY: 0

    - name: Discover the list of remote hypervisor public ssh keys to be imported
      find:
        path: "{{ ansible_facts['env']['HOME'] }}/.ssh"
        patterns: "*.pub"
        file_type: file
      register: _remote_ssh_public_keys

    - name: Fetch the remote hypervisor's public ssh keys
      fetch:
        src: "{{ item.path }}"
        dest: "{{ src_workspace }}/"
      loop: "{{ _remote_ssh_public_keys.files }}"
      loop_control:
        label: "{{ item.path }}"
      register: _remote_ssh_public_keys_fetch

    - name: Wait until boot completes then finalise access
      vars:
        _inventory: "{{ (_new_ir_inventory.stdout | from_json)['_meta']['hostvars'] }}"
        _running_servers: "{{ _virsh_snapshot_manifest_content.servers | selectattr('state', 'equalto', 'running') | list | sort(attribute='name') }}"
      include_tasks: tasks/snapshit_import_boot_vms.yml
      with_items: "{{ _inventory | dict2items | sort(attribute='key') }}"
      loop_control:
        loop_var: infrared_host_name
      when: (infrared_host_name.key in _running_servers) or
            (infrared_host_name.value.original_name is defined and infrared_host_name.value.original_name in _running_servers)

    - name: Get and write to output the compose/puddle version
      vars:
        _inventory: "{{ (_new_ir_inventory.stdout | from_json)['_meta']['hostvars'] }}"
      when: _inventory['undercloud-0'] is defined
      block:
        - name: Get the compose/puddle version from the undercloud
          shell: >-
            ssh
            {{ _inventory['undercloud-0']['ansible_ssh_common_args'] }}
            -i {{ _inventory['undercloud-0']['ansible_ssh_private_key_file'] }}
            {{ _inventory['undercloud-0']['ansible_user'] }}@{{ _inventory['undercloud-0']['ansible_host'] }}
            'cat ~/core_puddle_version || echo "not found"'
          register: _remote_compose_version
          args:
            executable: /bin/bash
          delegate_to: localhost
        - name: Output the compose/puddle version
          debug:
            msg: "Build mark: core_puddle={{ _remote_compose_version.stdout }}"
