---
- name: Import snapshots of virtual machines
  hosts: "{{ (_run_condition | bool) | ternary('hypervisor', 'none') }}"
  gather_facts: yes
  any_errors_fatal: true
  vars:
    vm_prefix: "{{ provision.prefix | default('') }}"
    src_workspace: "{{ lookup('env', 'WORKSPACE') | default('/tmp', true) }}"
    ir_cmd: "{{ (lookup('env', 'VIRTUAL_ENV') != '') | ternary(lookup('env', 'VIRTUAL_ENV') ~ '/bin/infrared', 'infrared') }}"
    ir_home: "{{ lookup('env', 'IR_HOME') | default(lookup('env', 'HOME') ~ '/.infrared', true) }}"
  tasks:
    - name: Install required packages
      package:
        name: libguestfs-tools-c
        state: present

    - name: Get info about existing virt storage pools
      virt_pool:
        command: info
      register: _virt_pools

    - name: Set default image pool path
      set_fact:
        _virt_pools:
          pools:
            images:
              path: "/var/lib/libvirt/images"
      when: _virt_pools.pools.images is not defined

    - name: Get info about existing VM's
      virt:
        command: list_vms
      register: _virt_list

    - name: Create the list of existing VM's to remove
      set_fact:
        _vm_to_remove_list: "{{ _virt_list.list_vms | select('match', vm_prefix ~ provision.virsh.snapshot.servers) | list }}"

    - name: Stop any running VM's
      shell: |
        set -e
        RETURN_CODE=0
        if ! virsh domstate {{ item }} | grep -qw 'shut off'; then
          virsh destroy {{ item }}
          RETURN_CODE=2
        fi
        exit ${RETURN_CODE}
      args:
        executable: /bin/bash
      loop: "{{ _vm_to_remove_list }}"
      register: _vm_stop
      changed_when: _vm_stop.rc == 2
      failed_when: _vm_stop.rc not in [0, 2]

    - name: Wait for all VM's to be stopped
      command: |
        virsh domstate {{ item }}
      changed_when: False
      register: _vm_shutdown
      until: _vm_shutdown.stdout.find('shut off') != -1
      retries: 5
      delay: 60
      loop: "{{ _vm_to_remove_list }}"

    - name: Delete any disk images related to running VM's
      shell: |
        for vdisk in $(virsh domblklist {{ item }} | awk '/{{ item }}/ {print $2}'); do
          rm -f ${vdisk}
        done
      args:
        executable: /bin/bash
      loop: "{{ _vm_to_remove_list }}"

    - name: Undefine all running VM's
      virt:
        name: "{{ item }}"
        command: undefine
      failed_when: false
      loop: "{{ _vm_to_remove_list }}"

    - name: Read the exported manifest file
      slurp:
        path: "{{ provision.virsh.snapshot.path }}/manifest.json"
      register: _manifest_file

    - name: Register manifest content as a fact
      set_fact:
        _manifest_content: "{{ _manifest_file.content | b64decode | from_json }}"

    - name: Import the disk images
      copy:
        src: "{{ provision.virsh.snapshot.path }}/{{ item.path | basename }}"
        dest: "{{ _virt_pools.pools.images.path }}/"
        checksum: "{{ item.checksum }}"
        remote_src: yes
      loop: "{{ _manifest_content.files | selectattr('path', 'match', '.*qcow2$') | list }}"
      loop_control:
        label: "{{ item.path | basename }}"

    - name: Inspect the disk1 images to identify whether an operating system is present
      command: >-
        virt-inspector
        --add {{ _virt_pools.pools.images.path }}/{{ item.path | basename }}
      register: _virt_inspector
      changed_when: false
      failed_when: false
      loop: "{{ _manifest_content.files | selectattr('path', 'match', '.*-disk1\\.qcow2$') | list }}"
      loop_control:
        label: "{{ item.path | basename }}"

    - name: Inject the host ssh key into the VM disk image (if it has an operating system)
      command: >-
        virt-customize
        --add {{ _virt_pools.pools.images.path }}/{{ item.item.path | basename }}
        --root-password password:redhat
        --mkdir /root/.ssh
        --chmod 0700:/root/.ssh
        --ssh-inject root:file:/root/.ssh/id_rsa.pub
        --chmod 0700:/home/{{ vm_provision_user }}/.ssh
        --ssh-inject {{ vm_provision_user }}:file:/root/.ssh/id_rsa.pub
      when: item.stdout != "<?xml version=\"1.0\"?>\n<operatingsystems/>"
      loop: "{{ _virt_inspector.results }}"
      loop_control:
        label: "{{ item.item.path | basename }}"
      vars:
        vm_provision_user: "{{ (item.item.path | basename is match('undercloud-.*')) | ternary('stack', 'heat-admin') }}"

    - name: Import the VM configuration
      copy:
        src: "{{ provision.virsh.snapshot.path }}/{{ item.path | basename }}"
        dest: "/etc/libvirt/qemu/"
        checksum: "{{ item.checksum }}"
        remote_src: yes
      loop: "{{ _manifest_content.files | selectattr('path', 'match', '.*xml$') | list }}"
      loop_control:
        label: "{{ item.path | basename }}"

    - name: Change machine type to work on the current hypervisor
      xml:
        path: "/etc/libvirt/qemu/{{ item.path | basename }}"
        xpath: "/domain/os/type"
        attribute: "machine"
        value: "pc"
      loop: "{{ _manifest_content.files | selectattr('path', 'match', '.*xml$') | list }}"
      loop_control:
        label: "{{ item.path | basename }}"

    # We use command here, because the virt module needs the raw XML
    # and a lookup will not work remotely. We could slurp all the files,
    # but that's a lot of work for very little gain.
    - name: Define the VM's
      command: >-
        virsh define /etc/libvirt/qemu/{{ item.path | basename }}
      loop: "{{ _manifest_content.files | selectattr('path', 'match', '.*xml$') | list }}"
      loop_control:
        label: "{{ item.path | basename }}"

    - name: Start any VM's that were in a running state
      virt:
        name: "{{ item.name }}"
        command: start
        state: running
      loop: "{{ _manifest_content.servers | selectattr('state', 'equalto', 'running') | list }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Fetch the infrared workspace
      fetch:
        src: "{{ provision.virsh.snapshot.path }}/infrared-workspace.tgz"
        dest: "{{ src_workspace }}/"
        flat: yes

    - name: Import the infrared workspace
      shell: |-
        IR_ACTIVE_WORKSPACE=$({{ ir_cmd }} workspace list --active)
        IR_NEW_WORKSPACE="workspace_$(date +%Y-%m-%d_%H-%M-%S)"
        {{ ir_cmd }} workspace import {{ src_workspace }}/infrared-workspace.tgz --name ${IR_NEW_WORKSPACE}
      delegate_to: localhost
      args:
        executable: /bin/bash

    - name: Add the imported ssh key to the hypervisor
      authorized_key:
        user: root
        state: present
        key: "{{ lookup('file', ir_home ~ '/.workspaces/active/id_rsa.pub') }}"

    - name: Adjust the imported workspace inventory for the new hypervisor
      replace:
        path: "{{ ir_home }}/.workspaces/active/hosts"
        regexp: "{{ _manifest_content.hypervisor_fqdn | regex_escape() }}"
        replace: "{{ hostvars.hypervisor.ansible_fqdn }}"

    - name: Wait until the VM has an address
      shell: |-
        virsh_addr_raw=$(virsh domifaddr --domain {{ item.name }} --source arp | awk '$1 !~ /^Name|^-|^$/ {print $4}')
        echo ${virsh_addr_raw%/*}
      args:
        executable: /bin/bash
      register: _wait_start
      delay: 60
      retries: 30
      until:
        - _wait_start.stdout is defined
        - _wait_start.stdout != ''
      loop: "{{ _manifest_content.servers | selectattr('state', 'equalto', 'running') | list }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Wait until the VM SSH service is up
      wait_for:
        port: 22
        host: "{{ item.stdout_lines | first }}"
        search_regex: OpenSSH
      loop: "{{ _wait_start.results }}"
      loop_control:
        label: "{{ item.item.name }}"
