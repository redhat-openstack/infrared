---
- set_fact:
    topology_node: "{{ node.value }}"

#default: "/var/lib/libvirt/images"
- name: define disk pool path
  set_fact:
      disk_pool: "{{ (provision.disk.pool if (topology_node.disks.disk1.path == None) else topology_node.disks.disk1.path) if not provision.rbd.on else rbd_namespace_url_libguestfs }}"

#default: "images"
- name: define disk pool name
  set_fact:
      pool_name: "{{ provision.pool.name }}"

- block:
      #todo(yfried): move this to verification section (before execution)
      - name: verify swap is within disk1 size
        vars:
            disk1_min: 8G #GB
        fail:
            msg:
                error: "Not enough disk space for swap"
                node: "{{ topology_node.name }}"
                swap: "{{ topology_node.swap }}"
                disk_size: "{{ topology_node.disks.disk1.size }}"
        when: (topology_node.disks.disk1.size|filesize('M')|int - topology_node.swap|default(0)|int) < (disk1_min|filesize('M')|int)

      - name: check and create directory
        file:
          path: "{{ disk_pool }}"
          state: directory
          owner: root
          group: root
          mode: 0755
          follow: yes
        when: disk_pool | default(False)

      - name: Check volume pool
        command: >
          virsh pool-uuid "{{ pool_name }}"
        register: pool_check
        ignore_errors: true
        changed_when: false

      - name: Print pool_check status
        debug:
            msg: "pool check is: {{pool_check}}"

      - name: create the volume pool xml file
        template:
          src: volume_pool.xml.j2
          dest: "/tmp/volume_pool.xml"
        when: pool_check is failed

      - name: Define volume pool
        command: "virsh pool-define /tmp/volume_pool.xml"
        when: pool_check is failed

      - name: Start volume pool
        virt_pool:
          command: start
          state: active
          name: "{{ pool_name }}"

      # Cannot be done in the same start operation as above, apparently
      - name: Mark volume pool as autostart
        virt_pool:
          name: "{{ pool_name }}"
          autostart: "yes"

      # For disks usually it will be only 1 disk so the async will run on nodes rather than disks creation
      - name: create disk(s) from vm base image
        vars:
            net_scripts: /etc/sysconfig/network-scripts
            libguestfs_opts: "{{ ' --format=raw ' if provision.rbd.on }}"
            img_format: "{{ 'rbd' if provision.rbd.on else 'qcow2' }}"
            base_image_path: "{{ rbd_namespace_url_libguestfs if provision.rbd.on }}"
            golden_image_name: "{{ provision.rbd.clonename }}@virsh"
        shell: |
            set -ex
            export LIBGUESTFS_PATH=/opt/appliance
            export LIBGUESTFS_BACKEND=direct

            {% for num in topology_node.node_indexes %}
            {% for disk_name, disk_values in topology_node.disks.items() %}
            {% set node_image = '{0}-{1}-{2}.{3}'.format(topology_node.name, num, disk_name,img_format) %}
            {% set image_name = (disk_values.import_url|default('') or topology_node.import_url|default('') or golden_image_name|default('')  or url) | basename %}
            {% if not disk_values.import_url %}

            ## Prepare disk image

            {% if disk_name == 'disk1' and topology_node.deploy_os|default(True) and topology_node.supports_libguestfs|default(True) -%}

            {% if provision.rbd.on %}
            if (rbd clone {{ rbd_clone_url }}@virsh {{ rbd_namespace_url }}/{{ node_image }}); then
              rbd resize --size {{ disk_values.size }} {{ rbd_namespace_url }}/{{ node_image }}
            fi
            {% endif %}

            echo "Image name is {{ image_name }}"

            # expand (copy-content) into new image in case of system disk
            # virt-resize needs target file with expected size to already exists
            os_version=`virt-cat {{ libguestfs_opts }} -a {{ base_image_path }}/{{ image_name }} /etc/os-release | awk '/^VERSION=/ {print $1}' | grep -oP '[0-9]+' | head -1`
            # Try to find the "root" partition or an xfs partition. Defaults to /dev/sda1 else.
            root_part=`virt-filesystems {{ libguestfs_opts }} --csv --long --no-title -a {{ base_image_path }}/{{ image_name }} | awk -F "," '{ if ($4 == "root") {print $1; exit;} else if ($3 == "xfs") {print $1; exit;} }'`
            root_part=${root_part:-/dev/sda1}
            # TODO replace base_imagE_path with rbd_clone_url@virsh everywhere here
            virt-resize {{ libguestfs_opts }} --expand $root_part {{ base_image_path }}/{{ image_name }} {{ disk_pool }}/{{ node_image }}
            # We need to regenerate a new machine-id due to DHCP IPv6 not working otherwise
            virt-sysprep {{ libguestfs_opts }} -a {{ disk_pool }}/{{ node_image }} \
                --operations machine-id
            # inject DEFROUTE for IPv6 and IPv6 to file and set it on for external network only
            NIC_DEFAULT="DEVICE=\"eth0\"\nBOOTPROTO=\"dhcp\"\nBOOTPROTOv6=\"dhcp\"\nONBOOT=\"yes\"\nTYPE=\"Ethernet\"\nUSERCTL=\"yes\"\nPEERDNS=\"yes\"\nIPV6INIT=\"yes\"\nPERSISTENT_DHCLIENT=\"1\"\n"
            virt-customize {{ libguestfs_opts }} -a {{ disk_pool }}/{{ node_image }} --selinux-relabel \
            {% for interface in topology_node.interfaces %}
                {% if loop.index0 == 0 %}
                    --run-command "echo -e \"$NIC_DEFAULT\" > {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}" \
                    --run-command 'grep ^DEFROUTE {{ net_scripts }}/ifcfg-eth{{ loop.index0 }} || echo "DEFROUTE=yes" >> {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' \
                    --run-command 'grep ^IPV6_DEFROUTE {{ net_scripts }}/ifcfg-eth{{ loop.index0 }} || echo "IPV6_DEFROUTE=yes" >> {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' \
                    {% if provision.host.network.mtu.size %} --run-command 'echo MTU={{ provision.host.network.mtu.size }} >> {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}'{% endif %}
                {%- else -%}--run-command 'cp {{ net_scripts }}/ifcfg-eth{0,{{ loop.index0 }}} && sed -i s/DEVICE=.*/DEVICE=eth{{ loop.index0 }}/g {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' {% endif %} \
                --run-command 'sed -i s/DEFROUTE=.*/DEFROUTE={{ 'yes' if (topology_node.external_network.network == interface.network) else 'no' }}/g {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' \
            {% endfor %}
            {%- if topology_node.swap|default('0')|int > 0 -%}
                --run-command 'dd if=/dev/zero of=/swapfile bs=1M count={{ topology_node.swap }}' \
                --run-command 'chmod 600 /swapfile' \
                --run-command 'mkswap /swapfile' \
                --run-command 'echo /swapfile none swap defaults 0 0 >> /etc/fstab' \
            {% endif -%}{# endif swap #}
                --selinux-relabel
            {% else %}{# we do not work with deploy_os enabled diskX - e.g. ceph disks #}
                rbd create --size {{ disk_values.size }} {{ rbd_namespace_url }}/{{ node_image }}
            {% endif %}{# endif disk1 #}

            # todo(yfried): use builtin module for chown
            {% else %}{# else import_url #}
            # in case of import simply copy that image to use as a disk
            cp -rf {{ base_image_path }}/{{ image_name }} {{ disk_pool }}/{{ node_image }}
            {% endif %}{# endif not import_url #}
            {% if not provision.rbd.on %}
            chown qemu:qemu {{ disk_pool }}/{{ node_image }}
            {% endif %}
            {% endfor %}{# endfor vm disks #}
            {% endfor %}{# endfor vm-type(controller etc) #}
      #  todo(yfried) needs a "creates"
      #  args:
      #      creates: "create disk(s) from vm base image"
        register: vm_disks
        async: 7200
        delay: 5
        poll: 0

      - name: save disks jids
        set_fact:
            async_disks: "{{ async_disks|default([]) + [vm_disks.ansible_job_id] }}"

  when: topology_node.disks|count > 0
