---
- set_fact:
    topology_node: "{{ node.value }}"

- name: define disk pool path
  set_fact:
      disk_pool: "{{ provision.disk.pool if (topology_node.disks.disk1.path == None) else topology_node.disks.disk1.path }}"

- name: define disk pool name
  set_fact:
      pool_name: "{{ provision.pool.name }}"

- block:
      #todo(yfried): move this to verification section (before execution)
      - name: verify swap is within disk1 size
        vars:
            disk1_min: 8G #GB
        fail:
            msg:
                error: "Not enough disk space for swap"
                node: "{{ topology_node.name }}"
                swap: "{{ topology_node.swap }}"
                disk_size: "{{ topology_node.disks.disk1.size }}"
        when: (topology_node.disks.disk1.size|filesize('M')|int - topology_node.swap|default(0)|int) < (disk1_min|filesize('M')|int)

      - name: check and create directory
        file:
          path: "{{ disk_pool }}"
          state: directory
          owner: root
          group: root
          mode: 0755
          follow: yes
        when: disk_pool | default(False)


      - name: Check volume pool
        command: >
          virsh pool-uuid "{{ pool_name }}"
        register: pool_check
        ignore_errors: true
        changed_when: false

      - name: create the volume pool xml file
        template:
          src: volume_pool.xml.j2
          dest: "/tmp/volume_pool.xml"
        when: pool_check is failed

      - name: Define volume pool
        command: "virsh pool-define /tmp/volume_pool.xml"
        when: pool_check is failed

      - name: Start volume pool
        virt_pool:
          command: start
          state: active
          autostart: "yes"
          name: "{{ pool_name }}"

      # For disks usually it will be only 1 disk so the async will run on nodes rather than disks creation
      - name: create disk(s) from vm base image
        vars:
            net_scripts: /etc/sysconfig/network-scripts
        shell: |
            set -ex
            export LIBGUESTFS_PATH=/opt/appliance
            export LIBGUESTFS_BACKEND=direct

            {% for num in topology_node.node_indexes %}
            {% for disk_name, disk_values in topology_node.disks.items() %}
            {% set node_image = '{0}-{1}-{2}.qcow2'.format(topology_node.name, num, disk_name) %}
            {% set image_name = (disk_values.import_url|default('') or topology_node.import_url|default('') or url) | basename %}
            {% if not disk_values.import_url %}

            # create empty disk
            qemu-img create -f qcow2 -o preallocation={{ disk_values.preallocation }} {{ disk_pool  }}/{{ node_image }} {{ disk_values.size }}

            ## Prepare disk image

            {% if disk_name == 'disk1' and topology_node.deploy_os|default(True) and topology_node.supports_libguestfs|default(True) -%}
            # expand (copy-content) into new image in case of system disk
            # virt-resize needs target file with expected size to already exists
            os_version=`virt-cat -a {{ base_image_path }}/{{ image_name }} /etc/os-release | awk '/^VERSION=/ {print $1}' | grep -oP '[0-9]+' | head -1`
            virt-resize --expand /dev/sda1 {{ base_image_path }}/{{ image_name }} {{ disk_pool }}/{{ node_image }}
            # We need to regenerate a new machine-id due to DHCP IPv6 not working otherwise
            virt-sysprep -a {{ disk_pool }}/{{ node_image }} \
                --operations machine-id
            # inject DEFROUTE for IPv6 and IPv6 to file and set it on for external network only
            virt-customize -a {{ disk_pool }}/{{ node_image }} \
            {% for interface in topology_node.interfaces %}
                {% if loop.index0 == 0 %}
                    --run-command 'grep ^DEFROUTE {{ net_scripts }}/ifcfg-eth{{ loop.index0 }} || echo "DEFROUTE=yes" >> {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' \
                    --run-command 'grep ^IPV6_DEFROUTE {{ net_scripts }}/ifcfg-eth{{ loop.index0 }} || echo "IPV6_DEFROUTE=yes" >> {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}'
                {%- else -%}--run-command 'cp {{ net_scripts }}/ifcfg-eth{0,{{ loop.index0 }}} && sed -i s/DEVICE=.*/DEVICE=eth{{ loop.index0 }}/g {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' {% endif %} \
                --run-command 'sed -i s/DEFROUTE=.*/DEFROUTE={{ 'yes' if (topology_node.external_network.network == interface.network) else 'no' }}/g {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' \
            {% endfor %}
            {%- if topology_node.swap|default('0')|int > 0 -%}
                --run-command 'dd if=/dev/zero of=/swapfile bs=1M count={{ topology_node.swap }}' \
                --run-command 'chmod 600 /swapfile' \
                --run-command 'mkswap /swapfile' \
                --run-command 'echo /swapfile none swap defaults 0 0 >> /etc/fstab' \
            {% endif -%}{# endif swap #}
                --selinux-relabel
            {% endif %}{# endif disk1 #}

            # todo(yfried): use builtin module for chown
            {% else %}{# else import_url #}
            # in case of import simply copy that image to use as a disk
            cp -rf {{ base_image_path }}/{{ image_name }} {{ disk_pool }}/{{ node_image }}
            {% endif %}{# endif not import_url #}
            chown qemu:qemu {{ disk_pool }}/{{ node_image }}
            {% endfor %}{# endfor vm disks #}
            {% endfor %}{# endfor vm-type(controller etc) #}
      #  todo(yfried) needs a "creates"
      #  args:
      #      creates: "create disk(s) from vm base image"
        register: vm_disks
        async: 7200
        poll: 0

      - name: save disks jids
        set_fact:
            async_disks: "{{ async_disks|default([]) + [vm_disks.ansible_job_id] }}"

  when: topology_node.disks|count > 0
