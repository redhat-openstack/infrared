---
- set_fact:
    topology_node: "{{ node.value }}"

- name: define disk pool path
  set_fact:
      disk_pool: "{{ provision.disk.pool if (topology_node.disks.disk1.path == None) else topology_node.disks.disk1.path }}"

- block:
      #todo(yfried): move this to verification section (before execution)
      - name: verify swap is within disk1 size
        vars:
            disk1_min: 8G #GB
        fail:
            msg:
                error: "Not enough disk space for swap"
                node: "{{ topology_node.name }}"
                swap: "{{ topology_node.swap }}"
                disk_size: "{{ topology_node.disks.disk1.size }}"
        when: (topology_node.disks.disk1.size|filesize('M')|int - topology_node.swap|default(0)|int) < (disk1_min|filesize('M')|int)

      - name: check and create directory
        file:
          path: "{{ disk_pool }}"
          state: directory
          owner: root
          group: root
          mode: 0755
          follow: yes
        when: disk_pool | default(False)


      - name: Check volume pool
        command: >
          virsh pool-uuid "{{ pool_name }}"
        register: pool_check
        ignore_errors: true
        changed_when: false

      - name: create the volume pool xml file
        template:
          src: volume_pool.xml.j2
          dest: "/tmp/volume_pool.xml"
        when: pool_check is failed

      - name: Define volume pool
        command: "virsh pool-define /tmp/volume_pool.xml"
        when: pool_check is failed

      - name: Start volume pool
        virt_pool:
          command: start
          state: active
          name: "{{ pool_name }}"

      # Cannot be done in the same start operation as above, apparently
      - name: Mark volume pool as autostart
        virt_pool:
          name: "{{ pool_name }}"
          autostart: "yes"

      # For disks usually it will be only 1 disk so the async will run on nodes rather than disks creation
      - name: create disk(s) from vm base image
        vars:
            net_scripts: /etc/sysconfig/network-scripts
            libguestfs_opts: "{{ ' --format=raw ' if provision.rbd.on }}"
            img_format: "{{ 'rbd' if provision.rbd.on else 'qcow2' }}"
            base_image_path: "{{ rbd_namespace_url_libguestfs if provision.rbd.on }}"
            #golden_image_name: "{{ provision.rbd.clonename }}@virsh"
        shell: |
            set -exo pipefail
            export LIBGUESTFS_PATH=/opt/appliance
            export LIBGUESTFS_BACKEND=direct

            {% for num in topology_node.node_indexes %}
            {% for disk_name, disk_values in topology_node.disks.items() %}
            {% set node_image = '{0}-{1}-{2}.qcow2'.format(topology_node.name, num, disk_name) %}
            {% set image_name = (disk_values.import_url|default('') or topology_node.import_url|default('') or url) | basename %}
            {% if not disk_values.import_url %}

            # create empty disk
            qemu-img create -f qcow2 -o preallocation={{ disk_values.preallocation }} {{ disk_pool  }}/{{ node_image }} {{ disk_values.size }}

            ## Prepare disk image

            {% if disk_name == 'disk1' and topology_node.deploy_os|default(True) and topology_node.supports_libguestfs|default(True) -%}

            device={{ rbd_namespace_url }}/{{ node_image }}

            {% if provision.rbd.on %}
            # if (rbd clone {{ rbd_clone_url }}@virsh {{ rbd_namespace_url }}/{{ node_image }}); then
            rbd clone {{ rbd_clone_url }}@virsh {{ rbd_namespace_url }}/{{ node_image }} || true
            rbd resize --size {{ disk_values.size }} {{ rbd_namespace_url }}/{{ node_image }} || true
            if (rbd showmapped | grep "{{ node_image }}" |  awk -F " " '{print $4; exit;}' == {{ node_image }}); then
              rbd_dev=`rbd showmapped | grep "{{ node_image }}" |  awk -F " " '{print $6; exit;}'`
            else
              rbd_dev=`rbd map {{ rbd_namespace_url }}/{{ node_image }}`
            fi
            #fi
            device=$rbd_dev
            {% endif %}

            echo "Image name is {{ image_name }}"
            echo "device var is $device"
            #exit 1

            # Try to find the "root" partition or an xfs partition. Defaults to /dev/sda1 else.
            root_part=`virt-filesystems {{ libguestfs_opts }} --csv --long --no-title -a $device | awk -F "," '{ if ($4 == "root") {print $1; exit;} else if ($3 == "xfs") {print $1; exit;} }'`
            root_part=${root_part:-/dev/sda1}
            # TODO replace base_imagE_path with rbd_clone_url@virsh everywhere here
            # TODO2: virt-resize will fail raw disk and dest clone are the same size
            # if disk_values.size not {{ rbd_namespace_url }}/{{ node_image }}; only then resize
            #virt-resize {{ libguestfs_opts }} --expand $root_part {{ base_image_path }}/{{ image_name }} $device
            # We need to regenerate a new machine-id due to DHCP IPv6 not working otherwise
            virt-sysprep {{ libguestfs_opts }} -a $device \
                --operations machine-id
            # inject DEFROUTE for IPv6 and IPv6 to file and set it on for external network only
            NIC_DEFAULT="DEVICE=\"eth0\"\nBOOTPROTO=\"dhcp\"\nBOOTPROTOv6=\"dhcp\"\nONBOOT=\"yes\"\nTYPE=\"Ethernet\"\nUSERCTL=\"yes\"\nPEERDNS=\"yes\"\nIPV6INIT=\"yes\"\nPERSISTENT_DHCLIENT=\"1\"\n"
            virt-customize {{ libguestfs_opts }} -a $device --selinux-relabel \
            {% for interface in topology_node.interfaces %}
                {% if loop.index0 == 0 %}
                    --run-command "echo -e \"$NIC_DEFAULT\" > {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}" \
                    --run-command 'grep ^DEFROUTE {{ net_scripts }}/ifcfg-eth{{ loop.index0 }} || echo "DEFROUTE=yes" >> {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' \
                    --run-command 'grep ^IPV6_DEFROUTE {{ net_scripts }}/ifcfg-eth{{ loop.index0 }} || echo "IPV6_DEFROUTE=yes" >> {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' \
                    {% if provision.host.network.mtu.size %} --run-command 'echo MTU={{ provision.host.network.mtu.size }} >> {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}'{% endif %}
                {%- else -%}--run-command 'cp {{ net_scripts }}/ifcfg-eth{0,{{ loop.index0 }}} && sed -i s/DEVICE=.*/DEVICE=eth{{ loop.index0 }}/g {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' {% endif %} \
                --run-command 'sed -i s/DEFROUTE=.*/DEFROUTE={{ 'yes' if (topology_node.external_network.network == interface.network) else 'no' }}/g {{ net_scripts }}/ifcfg-eth{{ loop.index0 }}' \
            {% endfor %}
            {%- if topology_node.swap|default('0')|int > 0 -%}
                --run-command 'dd if=/dev/zero of=/swapfile bs=1M count={{ topology_node.swap }}' \
                --run-command 'chmod 600 /swapfile' \
                --run-command 'mkswap /swapfile' \
                --run-command 'echo /swapfile none swap defaults 0 0 >> /etc/fstab' \
            {% endif -%}{# endif swap #}
                --selinux-relabel
            {% else %}{# we do not work with deploy_os enabled diskX - e.g. ceph disks #}
                # todo: with RBD, we dont want to create anything, just clone from golden images = even dummy VMs
                rbd create --size {{ disk_values.size }} {{ rbd_namespace_url }}/{{ node_image }}
            {% endif %}{# endif disk1 #}

            # todo(yfried): use builtin module for chown
            {% else %}{# else import_url #}
            # in case of import simply copy that image to use as a disk
            cp -rf {{ base_image_path }}/{{ image_name }} {{ disk_pool }}/{{ node_image }}
            {% endif %}{# endif not import_url #}
            chown qemu:qemu {{ disk_pool }}/{{ node_image }}
            {% endfor %}{# endfor vm disks #}
            {% endfor %}{# endfor vm-type(controller etc) #}
      #  todo(yfried) needs a "creates"
      #  args:
      #      creates: "create disk(s) from vm base image"
        register: vm_disks
        async: 7200
        poll: 0

      - name: save disks jids
        set_fact:
            async_disks: "{{ async_disks|default([]) + [vm_disks.ansible_job_id] }}"

  when: topology_node.disks|count > 0
