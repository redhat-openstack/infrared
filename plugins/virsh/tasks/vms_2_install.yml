---
# save current node to "{{ node_dict }}"
- include_vars:
    file: "{{ node.key }}"
    name: node_dict

- include_vars:
    file: "{{ provision.topology.network }}"
    name: network_info

- name: merge overriden settings to the separate var
  set_fact:
      topology_node: "{{ node_dict|default({}) | combine((override|default({})).get(node.key|basename|splitext|first, {}), recursive=True) }}"

- name: update node network info if needed
  set_fact:
      topology_node: "{{ topology_node | combine(network_info.nodes[topology_node.name]|default(network_info.nodes.default), recursive=True) }}"
  when: not topology_node.interfaces

- name: Set default node_start_index
  set_fact:
      node_start_index: 0

- name: Calculate node_start_index
  set_fact:
      node_start_index: "{{ node_start_index|int + 1 if (topology_node.name + '-' in item) and hostvars[item].get('ansible_connection') == 'ssh' else node_start_index|int }}"
  with_items: "{{ groups.all }}"
  when: provision.topology.extend | default(False)

  # This is an early inventory host registration.
  # It has 'local' connection type because has no configured network yet.
  # This is needed for cases when provisioning failed before we generated inventory file. In this cases we have orphan resources (vms and networks).
  # Early inventory generation adds host to inventory and generates new file right before virt-install.
  # If host in inventory but there is no corresponding  VM it will not cause a fail.
- name: add hosts to host list
  add_host:
      name: "{{ topology_node.name }}-{{ item }}"
      ansible_connection: "local"
  with_sequence: start={{ node_start_index|int }} end={{ node.value|int + node_start_index|int - 1 }}

- include_role:
      name: inventory-update
  delegate_to: localhost
  vars:
      inventory_file_name: 'hosts-prov'

# For installation the parallel run should be on the creation of the VMs as the amount is what needs to be paralleled
- name: create vm's
  shell: |
      virt-install --name {{ topology_node.name }}-{{ item }} \
          {% if topology_node.disks|count > 0 %}
          {% for disk_name, disk_values in topology_node.disks.iteritems() %}
          {% if disk_values.import_url is defined and disk_values.import_url %}
           --disk path={{ base_image_path }}/{{ topology_node.name }}-{{ item }}-{{ disk_name }}.qcow2,device=disk,bus=virtio,format=qcow2,cache={{ disk_values.cache }} \
          {% else %}
           --disk path={{ disk_values.path }}/{{ topology_node.name }}-{{ item }}-{{ disk_name }}.qcow2,device=disk,bus=virtio,format=qcow2,cache={{ disk_values.cache }} \
          {% endif -%}
          {% endfor %}
          {% else %}
          --disk none \
          {% endif %}
          --boot {{ provision.bootmode }} \
          {% for interface in topology_node.interfaces %}
           --network {{ 'bridge' if (interface.bridged|default(False)) else 'network' }}:{{ interface.network }} \
          {% endfor -%}
           --virt-type kvm \
           --cpu {{ topology_node.cpumodel|default('host-model') }} \
           --ram {{ topology_node.memory }} \
           --vcpus {{ topology_node.cpu }} \
           --os-variant {{ topology_node.os.variant }} \
           --import \
           --noautoconsole \
           --autostart \
           --vnc \
           --rng /dev/urandom
  with_sequence: start={{ node_start_index|int }} end={{ node.value|int + node_start_index|int - 1 }}
  register: vm_install
  async: 7200
  poll: 0

- name: save install jids
  set_fact:
      async_install: "{{ async_install|default([]) + vm_install.results|map(attribute='ansible_job_id')|list }}"
