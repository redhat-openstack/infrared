---
- set_fact:
    topology_node: "{{ node.value }}"

- name: define disk pool path
  set_fact:
      disk_pool: "{{ provision.disk.pool if (topology_node.disks.disk1.path == None) else topology_node.disks.disk1.path }}"

  # This is an early inventory host registration.
  # It has 'local' connection type because has no configured network yet.
  # This is needed for cases when provisioning failed before we generated inventory file. In this cases we have orphan resources (vms and networks).
  # Early inventory generation adds host to inventory and generates new file right before virt-install.
  # If host in inventory but there is no corresponding  VM it will not cause a fail.
- name: add hosts to host list
  add_host:
      name: "{{ topology_node.name }}-{{ item }}"
      ansible_connection: "local"
  with_items: "{{ topology_node.node_indexes }}"

- include_role:
      name: inventory-update
      apply:
          delegate_to: localhost
  vars:
      inventory_file_name: 'hosts-prov'

- name: create VM bridges for interfaces that don't belong to any network
  include_tasks: create_bridges.yml
  when: item.bridged|default(false)
  with_items: "{{ topology_node.interfaces }}"

- name: Set serial_files_dir fact
  set_fact:
    serial_files_dir: /var/lib/libvirt/qemu/

# For installation the parallel run should be on the creation of the VMs as the amount is what needs to be paralleled
- name: create vm's
  shell: |
      virt-install --name {{ topology_node.name }}-{{ item }} \
          {% if topology_node.disks|count > 0 %}
          {% for disk_name, disk_values in topology_node.disks.items() %}
          {% if disk_values.import_url is defined and disk_values.import_url %}
           --disk path={{ base_image_path }}/{{ topology_node.name }}-{{ item }}-{{ disk_name }}.qcow2,device=disk,bus=
           {%- if topology_node.disks.disk1.bus is defined %}{{ topology_node.disks.disk1.bus }}{% else %}{{ provision.disk.bus }}{% endif %},format=qcow2,cache={{ disk_values.cache }} \
          {% else %}
           --disk path={{ disk_pool }}/{{ topology_node.name }}-{{ item }}-{{ disk_name }}.qcow2,device=disk,bus={{ provision.disk.bus }},format=qcow2,cache={{ disk_values.cache }} \
          {% endif -%}
          {% endfor %}
          {% else %}
          --disk none \
          {% endif %}
          {% if provision.bootmode == 'uefi' %}
          --boot {{ 'hd' if topology_node.deploy_os|default(True) else 'uefi' }} \
          {% else %}
          --boot {{ provision.bootmode }} \
          {% endif %}
          {% for interface in topology_node.interfaces %}
           --network {{ 'bridge' if (interface.bridged|default(False)) else 'network' }}:{{ prefix|default('') }}{{ (prefix is defined|ternary('-','')) }}
           {%- if interface.needs_formatting | default(False) %}{{ interface.network | format(item) }}{% else %}{{ interface.network }}{% endif %}
           {%- if interface.model is defined and interface.model %},model={{ interface.model }}{% endif %}
           {%- if interface.portgroup is defined %},portgroup={{ interface.portgroup }}{% endif %} \
          {% endfor -%}
          {% if provision.serial.files %}
           --serial file,path="{{ serial_files_dir }}/{{ prefix|default('') }}{{ topology_node.name }}-{{ item }}.log" \
          {% endif %}
           --virt-type kvm \
           --cpu {{ topology_node.cpumodel|default('host-passthrough') }} \
           --ram {{ topology_node.memory }} \
           --vcpus {{ topology_node.cpu }} \
           --os-variant {{ topology_node.os.variant }} \
           --import \
           --graphics vnc,listen={{ provision.vnc.external | ternary('0.0.0.0','127.0.0.1') }},password={{ provision.vnc.password if provision.vnc.password is defined else '' }} \
           --noautoconsole \
           --autostart \
           --rng /dev/urandom {{ provision.get('virtopts', '') }}
  with_items: "{{ topology_node.node_indexes }}"
  register: vm_install
  async: 7200
  poll: 0

- name: save install jids
  set_fact:
      async_install: "{{ async_install|default([]) + vm_install.results|map(attribute='ansible_job_id')|list }}"
