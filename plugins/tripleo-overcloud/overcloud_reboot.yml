#
# Reboot overcloud nodes one-by-one.
#
- name: Overcloud Reboot
  hosts: overcloud_nodes:!unused
  gather_facts: false
  any_errors_fatal: true
  serial: 1
  tasks:
      # Temporerily disable ceph rebalance
      - name: temporerily disable ceph rebalance
        become: true
        command: "{{ item }}"
        when: "'ceph' in group_names"
        delegate_to: "{{ groups['controller']|first }}"
        with_items:
            - 'ceph osd set noout'
            - 'ceph osd set norebalance'

      # check pcs cluster status
      - name: check pacemaker cluster status
        become: true
        shell: pcs status 2>&1 | grep 'cluster is not currently running on this node'
        register: pcs_active
        ignore_errors: true

      - name: check this if this is pcmk-remote node
        become: true
        shell: |
            pcs status 2>&1 | grep -w 'RemoteOnline:' | grep {{ inventory_hostname }}
        register: pcmk_remote
        ignore_errors: true
        when: pcs_active|failed

      - name: stop pcs cluster
        become: true
        command: "pcs cluster stop --request-timeout=300 {% if groups['controller']|length  < 3 %} --force {% endif %}"
        when:
            - pcs_active|failed
            - pcmk_remote|failed

      - name: reboot the node
        become: true
        shell: "sleep 5 && shutdown -r now"
        async: 1
        poll: 0
        ignore_errors: true

      - name: wait for node to go down
        command: ping -c1 {{ ansible_ssh_host }}
        register: node_down
        until: node_down.rc != 0
        retries: 60
        delay: 3
        ignore_errors: true
        delegate_to: hypervisor

      - block:
            - name: waiting for the node to be available
              wait_for:
                  port: 22
                  host: "{{ ansible_ssh_host }}"
                  search_regex: OpenSSH
                  delay: 60
                  timeout: 360
              delegate_to: localhost
              when: "'hypervisor' not in groups"

            - name: waiting for the node to be available
              become: no
              wait_for:
                  host: "{{ ansible_ssh_host }}"
                  port: 22
                  search_regex: OpenSSH
                  delay: 30
                  sleep: 15
                  timeout: 360
              delegate_to: hypervisor
              when: "'hypervisor' in groups"

      - name: check node is ssh-able
        shell: id || echo "NOT_READY"
        register: id_ssh
        until: "'{{ ansible_ssh_user }}' in id_ssh.stdout"
        retries: 20
        delay: 15

      - name: check pcmk_remote node is up
        become: true
        shell: pcs status | grep -w 'RemoteOnline:'
        register: remote_online
        until: "'{{ inventory_hostname }}' in remote_online.stdout"
        retries: 6
        delay: 5
        when:
            - pcs_active|failed
            - pcmk_remote|succeeded

      # pacemaker managed services
      - name: start pcs cluster on a node
        become: true
        command: pcs cluster start --wait=300
        when:
            - pcs_active|failed
            - pcmk_remote|failed

      - name: wait for pacemaker node to recover
        become: true
        shell: pcs status | grep -w 'Online:'
        register: pcs_status
        until: "'{{ inventory_hostname }}' in pcs_status.stdout"
        retries: 6
        delay: 5
        when:
            - pcs_active|failed
            - pcmk_remote|failed

      - name: cleanup pacemaker resource
        shell: |
            pcs resource restart {{ item }} ;
            pcs resource cleanup {{ item }}
        become: true
        ignore_errors: true
        with_items:
          - "{{ cleanup_services }}"
        when:
          - cleanup_services is defined
          - cleanup_services|default({})|length > 0
          - pcs_active|failed
          - pcmk_remote|failed

      # Todo(yprokule): enhance search patterns
      - name: check for any stopped pcs resources
        become: true
        shell: |
            pcs status | grep {{ item }} || /bin/true
        register: srv_status
        until: srv_status.stdout.find("{{ item }}") == -1
        ignore_errors: true
        retries: 36
        delay: 5
        with_items:
            - 'Stopped:'
        when:
            - pcs_active|failed
            - pcmk_remote|failed
            - "'{{ install.deployment.files | basename }}' == 'virt'"

      - name: Report pacemaker status for stopped resources
        become: true
        register: fail_stop
        command: pcs status
        when:
            - srv_status|failed

      - name: Stopped resources found
        fail:
            msg: Some pacemeker resource failed to come back online after reboot
        when:
            - fail_stop|changed


      # Todo(yprokule): enhance search patterns
      - name: Check for any unmanaged pcs resources
        become: true
        shell: |
            pcs status | grep {{ item }} || /bin/true
        register: srv_status
        until: srv_status.stdout.find("{{ item }}") == -1
        ignore_errors: true
        retries: 36
        delay: 5
        with_items:
            - 'unmanaged'
        when:
            - pcs_active|failed
            - pcmk_remote|failed

      - name: Report pacemaker status for unmanaged resources
        become: true
        register: fail_unmanaged
        command: pcs status
        when:
            - srv_status|failed

      - name: Unmanged resources found
        fail:
            msg: Some pacemeker resource(s) unmanged after reboot
        when:
            - fail_unmanaged|changed

      # Re-enable ceph rebalance
      - name: reenable ceph rebalance
        become: true
        command: "{{ item }}"
        when: "'ceph' in group_names"
        delegate_to: "{{ groups['controller']|first }}"
        with_items:
            - 'ceph osd unset noout'
            - 'ceph osd unset norebalance'

      - name: wait for OSDs to come back
        become: true
        command: ceph pg stat
        register: active_osd
        until: active_osd.stdout.find("active+clean") > -1
        retries: 24
        delay: 15
        when: "'ceph' in group_names"
        delegate_to: "{{ groups['controller']|first }}"

      - name: check clock_skew on ceph cluster
        become: true
        shell: |
            ceph status | grep 'Monitor clock skew detected' || echo 'ALL_GOOD'
        register: clock_skew
        when: "'ceph' in group_names"
        delegate_to: "{{ groups['controller']|first }}"

      - name: check ceph cluster status
        become: true
        command: ceph status
        register: ceph_health
        until: ceph_health.stdout.find("HEALTH_OK") > -1
        retries: 30
        delay: 5
        delegate_to: "{{ groups['controller']|first }}"
        when:
            - "'ceph' in group_names"
            - "groups['ceph']|difference(groups['unused']|default([]))|length >= 3"
            - "'ALL_GOOD' in clock_skew"

      - name: check ceph cluster status with clock skew
        become: true
        command: ceph status
        register: ceph_health
        until: ceph_health.stdout.find("HEALTH_WARN") > -1 or ceph_health.stdout.find("HEALTH_OK") > -1
        retries: 30
        delay: 5
        delegate_to: "{{ groups['controller']|first }}"
        when:
            - "'ceph' in group_names"
            - "groups['ceph']|difference(groups['unused']|default([]))|length >= 3"
