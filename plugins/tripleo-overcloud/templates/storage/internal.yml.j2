parameter_defaults:
    CinderEnableIscsiBackend: false
    CinderEnableRbdBackend: true
    CinderEnableNfsBackend: false
    NovaEnableRbdBackend: true
    GlanceBackend: {{ install.glance.backend }}
    CinderRbdPoolName: "volumes"
    NovaRbdPoolName: "vms"
    GlanceRbdPoolName: "images"
{% if 0 < ( (install.storage.nodes|default(0)) or (groups['ceph']|default([])|length) or 1 ) < 3  %}
    CephPoolDefaultSize: {{ (install.storage.nodes|default(0)) or (groups['ceph']|default([])|length) or 1 }}
{% endif %}
{% if install.containers|default(false) and install.version|default(undercloud_version)|openstack_release > 11 %}
{# containers uses ceph-ansible #}
    CephPoolDefaultPgNum: 32
    CephAnsibleDisksConfig:
        devices:
{% if install.splitstack|default(False) %}
            - '/dev/vdb'
{% else %}
{% if install.ceph.osd.type == 'bluestore' %}
{# at the moment, only lvm and bluestore are supported other scenarios will be deprecated #}
{# lvm will use non spinning osds for wal we cant simulate those in a virtual environment without a prescript  #}
{% for disk in storage_node_disks[1:] %}
            - '/dev/{{ disk }}'
{% endfor %}
        osd_scenario: lvm
        osd_objectstore: {{ install.ceph.osd.type }}
{% elif install.ceph.osd.type == 'filestore' %}
{%  if install.ceph.osd.scenario == 'non-collocated' %}
{% for disk in storage_node_disks[1:-1] %}
            - '/dev/{{ disk }}'
{% endfor %}

        dedicated_devices:
{% for disk in storage_node_disks[1:-1] %}
            - '/dev/{{ storage_node_disks[-1] }}'
{% endfor %}
{% elif install.ceph.osd.scenario == 'lvm' or install.ceph.osd.scenario == 'collocated' %}
{% for disk in storage_node_disks[1:] %}
            - '/dev/{{ disk }}'
{% endfor %}

{% endif %}{# filestore scenarios #}
        osd_scenario: {{ install.ceph.osd.scenario }}
        osd_objectstore: {{ install.ceph.osd.type }}

{% endif %}{#End of install.ceph_type = bluestore #}
{% endif %}{# end of install.splitstack #}
        journal_size: 512

{% else %}
    ExtraConfig:
      ceph::profile::params::osd_pool_default_pg_num: 32
      ceph::profile::params::osd_pool_default_pgp_num: 32
      ceph::profile::params::osds:
{% for disk in storage_node_disks[1:] %}
       '/dev/{{ disk }}': {}
{% endfor %}
{% endif %}
