parameter_defaults:
{% if install.version|default(undercloud_version)|openstack_release == 15%}
    EnableRhcs4Beta: true
{% endif %}
{% if install.version|default(undercloud_version)|openstack_release > 14%}
    CephClusterName: {{ install.ceph.cluster.name }}
{% endif %}
    CephAnsiblePlaybookVerbosity: 3
    CinderEnableIscsiBackend: false
    CinderEnableRbdBackend: true
    CinderEnableNfsBackend: false
    NovaEnableRbdBackend: true
    GlanceBackend: {{ install.glance.backend }}
    CinderRbdPoolName: "volumes"
    NovaRbdPoolName: "vms"
    GlanceRbdPoolName: "images"
{% if 0 < ( (install.storage.nodes|default(0)|int) or (groups['ceph']|default([])|length) or 1 ) < 3  %}
    CephPoolDefaultSize: {{ (install.storage.nodes|default(0)|int) or (groups['ceph']|default([])|length) or 1 }}
{% endif %}
{% if install.containers|default(false) and install.version|default(undercloud_version)|openstack_release > 11 %}
{# containers uses ceph-ansible #}
    CephPoolDefaultPgNum: {{ install.ceph.pgnum }}
    CephAnsibleDisksConfig:
        devices:
{% if install.splitstack|default(False) %}
            - '/dev/vdb'
        osd_scenario: lvm
        osd_objectstore: bluestore
{% else %}
{% if install.version|default(undercloud_version)|openstack_release >= 13%}
{% if install.ceph.osd.type|default('bluestore') == 'bluestore' %}
{% for disk in storage_node_disks[1:] %}
            - '/dev/{{ disk }}'
{% endfor %}
        osd_scenario: lvm
        osd_objectstore: bluestore
{% elif install.ceph.osd.type == 'filestore' %}{# not bluestore selected #}
{# handle the two filestore cases after declaration of osd types and scenarios #}
{% if install.ceph.osd.scenario|default('collocated') ==  'collocated' %}
{% for disk in storage_node_disks[1:] %}
            - '/dev/{{ disk }}'
{% endfor %}
{% elif install.ceph.osd.scenario == 'non-collocated' %}
{% for disk in storage_node_disks[1:-1] %}
            - '/dev/{{ disk }}'
{% endfor %}
        dedicated_devices:
{% for disk in storage_node_disks[1:-1] %}
            - '/dev/{{ storage_node_disks[-1] }}'
{% endfor %}
{% endif %}{# filestore-collocated or non-collocated #}
        osd_scenario: {{ install.ceph.osd.scenario }}
        osd_objectstore: {{ install.ceph.osd.type|default('collocated') }}
{% endif %} {# bluestore if #}
{% else %}{# version is smaller than 13. means version 12 because 11 and below are handled as puppet below #}
{# in v 12 will default to a collocated scenario utillizing all devices beyond vda explicitly #}
{% for disk in storage_node_disks[1:] %}
            - '/dev/{{ disk }}'
{% endfor %}
        osd_scenario: collocated
        journal_collocation: true
{% endif %}
{% endif %}{# end of install.splitstack #}

        journal_size: 512
{% if install.ceph.hci.memreserve|default(false) and install.version|default(undercloud_version)|openstack_release >= 13 %}
{# This block, for openstack>=13 defines the memory reservation feature introduced in ceph 3.2 #}
    CephAnsibleExtraConfig:
        is_hci: true
{% endif %}{# End of is_hci block #}
{% if install.ceph.rgw.swift.compatibility|default(false) and install.version|default(undercloud_version)|openstack_release >= 13 %}
{# This block add a ceph.conf override (under [global]) to force rgw to return same headers as swift and enable versioning#}
    CephConfigOverrides:
        rgw_swift_enforce_content_length: true
        rgw_swift_versioning_enabled: true
{% endif %}{# End of rgw_swift_enforce_content_length block #}
{% else %}
    ExtraConfig:
      ceph::profile::params::osd_pool_default_pg_num: {{ install.ceph.pgnum }}
      ceph::profile::params::osd_pool_default_pgp_num: {{ install.ceph.pgnum }}
      ceph::profile::params::osds:
{% for disk in storage_node_disks[1:] %}
       '/dev/{{ disk }}': {}
{% endfor %}
{% endif %}
