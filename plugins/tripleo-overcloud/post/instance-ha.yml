- name: Stop and disable openstack services and libvirtd
  hosts: compute
  become: yes
  any_errors_fatal: true
  tasks:
      - service:
            name: "{{ item }}"
            state: stopped
            enabled: no
        with_items:
            - neutron-openvswitch-agent
            - openstack-ceilometer-compute
            - openstack-nova-compute
            - libvirtd

- name: Generate authkey for remote pacemaker
  hosts: localhost
  tasks:
      - shell: "dd if=/dev/urandom of='{{ inventory_dir }}/authkey' bs=4096 count=1"

- name: Deploy auth key
  hosts: pacemaker:compute
  become: yes
  gather_facts: yes
  any_errors_fatal: true
  tasks:
      - name: Make sure pacemaker config dir exists
        file:
            path: /etc/pacemaker
            state: directory
            mode: 0755

      - name: Copy generated autkey to overcloud nodes
        copy:
            src: "{{ inventory_dir }}/authkey"
            dest: /etc/pacemaker/
            mode: 0644

- name: Enable and start remote_pacemaker on computes
  hosts: compute
  become: yes
  any_errors_fatal: true
  tasks:
      - name: Open pacemaker_remote port
        shell: "iptables -I INPUT -p tcp --dport 3121 -j ACCEPT"

      - name: Make created iptables rules persistant
        shell: "/sbin/service iptables save"

      - name: Open pacemaker_remote IPv6 port
        shell: "ip6tables -I INPUT -p tcp --dport 3121 -j ACCEPT"

      - name: Make created ip6tables rules persistant
        shell: "/sbin/service ip6tables save"

      - name: Start pacemaker remote service at compute nodes
        service:
            name: pacemaker_remote
            enabled: yes
            state: started

- name: Setup credentials
  hosts: undercloud
  gather_facts: yes
  any_errors_fatal: yes
  tasks:
      # for openstack v11 the OS_TENANT_NAME is replaced with OS_PROJECT_NAME
      # Trying to resolve OS_PROJECT_NAME and then OS_TENANT_NAME
      - name: grab auth data from openstackrc file and publish it as YAML
        vars:
        shell: |
            source ~/overcloudrc
            echo "
            auth_url: $OS_AUTH_URL
            username: $OS_USERNAME
            password: $OS_PASSWORD
            project_name: ${OS_PROJECT_NAME:-$OS_TENANT_NAME}
            "
            if [ -n "$OS_PROJECT_DOMAIN_NAME" ]; then
                echo "project_domain_name: $OS_PROJECT_DOMAIN_NAME"
            fi
            if [ -n "$OS_USER_DOMAIN_NAME" ]; then
                echo "user_domain_name: $OS_USER_DOMAIN_NAME"
            fi
        register: creds


#- name: Run setup as root, to get all Ansible facts
#  hosts: controller
#  gather_facts: yes
#  become: yes
#  become_method: sudo
#  any_errors_fatal: true
#  tasks:
#      - setup:

- name: Setup pacemaker for instance HA
  hosts: controller[0]
  gather_facts: yes
  become: yes
  vars:
      auth: "{{ hostvars['undercloud-0']['creds']['stdout'] | from_yaml }}"
  environment:
      OS_USERNAME: "{{ auth.username | default('admin') }}"
      OS_PASSWORD: "{{ auth.password }}"
      OS_AUTH_URL: "{{ auth.auth_url }}"
      OS_TENANT_NAME: "{{ auth.project_name | default('admin') }}"
  any_errors_fatal: true
  tasks:
      - name: Create NovaEvacuate resource
        shell: "pcs resource create nova-evacuate ocf:openstack:NovaEvacuate auth_url=$OS_AUTH_URL username=$OS_USERNAME password=$OS_PASSWORD tenant_name=$OS_TENANT_NAME no_shared_storage=0"

      - name: Create pacemaker constraints to start VIP resources before nova-evacuate
        shell: |
            for i in $(pcs status | grep IP | awk '{ print $1 }')
              do pcs constraint order start $i then nova-evacuate
            done

      - name: Create pacemaker constraints to start openstack services before nova-evacuate
        shell: "pcs constraint order start {{ item }} then nova-evacuate require-all=false"
        with_items:
            - openstack-glance-api-clone
            - neutron-metadata-agent-clone
            - openstack-nova-conductor-clone
        when: "{{ install.version|openstack_release }} < 10"

      - name: Disable keystone resource
        shell: "pcs resource disable openstack-keystone --wait=900"
        when: "{{ install.version|openstack_release }} < 9"

      # Keystone resource was replaced by openstack-core resource in RHOS9
      - name: Disable openstack-core resource
        shell: "pcs resource disable openstack-core --wait=900"
        when: "{{ install.version|openstack_release }} == 9"

      - name: Set controller pacemaker property on controllers
        shell: "pcs property set --node {{ hostvars[item]['ansible_hostname'] }} osprole=controller"
        with_items: "{{ groups['pacemaker'] }}"
        when: install.role is defined and install.role.files != 'default'

      - name: Get stonith devices
        shell: "pcs stonith | awk '{print $1}' | tr '\n' ' '"
        register: stonithdevs
        when: install.role is defined and install.role.files != 'default'

      - name: Setup stonith devices
        shell: |
            for i in $(sudo cibadmin -Q --xpath //primitive --node-path | awk -F "id='" '{print $2}' | awk -F "'" '{print $1}' | uniq); do
              found=0
              if [ -n "{{ stonithdevs.stdout }}" ]; then
                for x in {{ stonithdevs.stdout }}; do
                  if [ "$x" == "$i" ]; then
                    found=1
                  fi
                done
              fi
              if [ $found = 0 ]; then
                sudo pcs constraint location $i rule resource-discovery=exclusive score=0 osprole eq controller
              fi
            done
        when: install.role is defined and install.role.files != 'default'

      - name: Create compute pacemaker resources and constraints
        shell: |
            pcs resource create neutron-openvswitch-agent-compute systemd:neutron-openvswitch-agent --clone interleave=true --disabled --force
            pcs constraint location neutron-openvswitch-agent-compute-clone rule resource-discovery=exclusive score=0 osprole eq compute
            pcs resource create libvirtd-compute systemd:libvirtd --clone interleave=true --disabled --force
            pcs constraint location libvirtd-compute-clone rule resource-discovery=exclusive score=0 osprole eq compute
            pcs constraint order start neutron-openvswitch-agent-compute-clone then libvirtd-compute-clone
            pcs constraint colocation add libvirtd-compute-clone with neutron-openvswitch-agent-compute-clone
            pcs resource create ceilometer-compute systemd:openstack-ceilometer-compute --clone interleave=true --disabled --force
            pcs constraint location ceilometer-compute-clone rule resource-discovery=exclusive score=0 osprole eq compute
            pcs constraint order start libvirtd-compute-clone then ceilometer-compute-clone
            pcs constraint colocation add ceilometer-compute-clone with libvirtd-compute-clone

            pcs resource create nova-compute-checkevacuate ocf:openstack:nova-compute-wait auth_url=$OS_AUTH_URL username=$OS_USERNAME password=$OS_PASSWORD tenant_name=$OS_TENANT_NAME domain=localdomain op start timeout=300 --clone interleave=true --disabled --force
            pcs constraint location nova-compute-checkevacuate-clone rule resource-discovery=exclusive score=0 osprole eq compute

            pcs resource create nova-compute systemd:openstack-nova-compute op start timeout=60s --clone interleave=true --disabled --force
            pcs constraint location nova-compute-clone rule resource-discovery=exclusive score=0 osprole eq compute
            pcs constraint order start nova-compute-checkevacuate-clone then nova-compute-clone require-all=true

            pcs constraint order start nova-compute-clone then nova-evacuate require-all=false
            pcs constraint order start libvirtd-compute-clone then nova-compute-clone
            pcs constraint colocation add nova-compute-clone with libvirtd-compute-clone

      - name: Create pacemaker constraint for neutron-server, nova-conductor and ceilometer-notification
        shell: |
            pcs constraint order start neutron-server-clone then neutron-openvswitch-agent-compute-clone require-all=false
            pcs constraint order start openstack-ceilometer-notification-clone then ceilometer-compute-clone require-all=false
            pcs constraint order start openstack-nova-conductor-clone then nova-compute-checkevacuate-clone require-all=false
        when: "{{ install.version|openstack_release }} < 10"

      # TODO (mkrcmari) Create a play for baremetal computes fencing setup
      - name: Set up virt-fencing for compute nodes
        shell: |
            COMPUTENAME={{ hostvars[item]['ansible_hostname'] }}
            VM_UUID={{ hostvars[item]['ansible_product_uuid'] }}
            pcs stonith delete my-stonith-xvm-$COMPUTENAME || /bin/true
            pcs stonith create my-stonith-xvm-$COMPUTENAME fence_xvm port=$VM_UUID pcmk_host_list=$COMPUTENAME use_uuid=1 op monitor interval=30s
        with_items: "{{ groups['compute'] }}"
        when: "'hypervisor' in groups"

      - name: Create fence-nova pacemaker resource
        shell: "pcs stonith create fence-nova fence_compute auth-url=$OS_AUTH_URL login=$OS_USERNAME passwd=$OS_PASSWORD tenant-name=$OS_TENANT_NAME domain=localdomain record-only=1 no-shared-storage=False --force"

      - name: Create constraint for fence-nova in order not to stop whole control plane when cleaning fence-nova stonith device up
        shell: "pcs constraint location fence-nova rule resource-discovery=never score=0 osprole eq controller --force"

      - name: Create pacemaker constraint for fence-nova to start after galera
        shell: "pcs constraint order promote galera-master then fence-nova require-all=false"

      - name: Create pacemaker constraint for nova-fence to start after all nova resource started on older release with all services managed by pcs
        shell: "pcs constraint order start openstack-nova-conductor-clone then start fence-nova require-all=false"
        when: "{{ install.version|openstack_release }} < 10"

      - name: Create nova-compute order constraint on fence-nova
        shell: "pcs constraint order start fence-nova then nova-compute-clone"

      - name: Set cluster recheck interval to 1 minute
        shell: "pcs property set cluster-recheck-interval=1min"

      - name: Create pacemaker remote resource on compute nodes
        shell: "pcs resource create {{ hostvars[item]['ansible_hostname'] }} ocf:pacemaker:remote reconnect_interval=240 op monitor interval=20"
        with_items: "{{ groups['compute'] }}"

      - name: Set osprole for compute nodes
        shell: "pcs property set --node {{ hostvars[item]['ansible_hostname'] }} osprole=compute"
        with_items: "{{ groups['compute'] }}"

      - name: Add pacemaker stonith devices of compute nodes to level 1
        shell: "pcs stonith level add 1 {{ hostvars[item]['ansible_hostname'] }} my-stonith-xvm-{{ hostvars[item]['ansible_hostname'] }},fence-nova"
        with_items: "{{ groups['compute'] }}"

      - name: Enable keystone resource
        shell: "pcs resource enable openstack-keystone"
        when: "{{ install.version|openstack_release }} < 9"

      - name: Enable openstack-core resource
        shell: "pcs resource enable openstack-core"
        when: "{{ install.version|openstack_release }} == 9"

      - name: Wait for httpd service to be started
        shell: "systemctl show httpd --property=ActiveState"
        register: httpd_status_result
        until: httpd_status_result.stdout.find('inactive') == -1 and httpd_status_result.stdout.find('activating') == -1
        retries: 30
        delay: 10
        when: "{{ install.version|openstack_release }} > 8"

      - name: Enable compute resources
        shell: "pcs resource enable {{ item }}"
        with_items:
            - neutron-openvswitch-agent-compute
            - libvirtd-compute
            - ceilometer-compute
            - nova-compute-checkevacuate
            - nova-compute

  # Creation of neutron-openvswitch-agent resource may cause stopping the service on controllers since we do not
  # manage the service on controllers by pacemaker from RHOS10, make sure the service is started on controllers.
- name: Make sure that neutron-openvswitch-agent service is started
  hosts: pacemaker
  become: yes
  any_errors_fatal: true
  tasks:
      - service:
            name: neutron-openvswitch-agent
            state: started
        when: "{{ install.version|openstack_release }} > 9"

- name: Wait for libvirt and nova-compute resources to be available on compute nodes
  hosts: compute
  any_errors_fatal: true
  tasks:
      - name: Wait for nova-service service to be started
        shell: "systemctl show openstack-nova-compute --property=ActiveState"
        register: compute_status_result
        until: compute_status_result.stdout.find('inactive') == -1 and compute_status_result.stdout.find('activating') == -1
        retries: 600
        delay: 3

- name: Cleanup all pacemaker resource
  hosts: controller[0]
  become: yes
  any_errors_fatal: true
  tasks:
      - name: Cleanup all pacemaker resources
        shell: "pcs --force resource cleanup"
        when: "{{ install.version|openstack_release }} < 8"
