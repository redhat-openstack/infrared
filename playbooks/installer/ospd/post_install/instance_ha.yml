- name: Stop and disable openstack services and libvirtd
  hosts: compute
  become: yes
  tasks:
      - service:
            name: "{{ item }}"
            state: stopped
            enabled: no
        with_items:
            - neutron-openvswitch-agent
            - openstack-ceilometer-compute
            - openstack-nova-compute
            - libvirtd

- name: Generate authkey for remote pacemaker
  hosts: localhost
  tasks:
      - shell: "dd if=/dev/urandom of=authkey bs=4096 count=1"

- name: Deploy auth key
  hosts: controller:compute
  become: yes
  gather_facts: yes
  tasks:
      - name: Make sure pacemaker config dir exists
        file:
            path: /etc/pacemaker
            state: directory
            mode: 0755

      - name: Copy generated autkey to overcloud nodes
        copy:
            src: "{{ inventory_dir }}/authkey"
            dest: /etc/pacemaker/
            mode: 0644

- name: Enable and start remote_pacemaker on computes
  hosts: compute
  become: yes
  tasks:
      - name: Open pacemaker_remote port
        shell: "iptables -I INPUT -p tcp --dport 3121 -j ACCEPT"

      - name: Make created iptables rules persistant
        shell: "/sbin/service iptables save"

      - name: Start pacemaker remote service at compute nodes
        service:
            name: pacemaker_remote
            enabled: yes
            state: started

- name: Get the overcloud rc file
  hosts: undercloud
  gather_facts: no
  become: yes
  become_user: "{{ installer.user.name }}"
  tasks:
      - fetch:
            src: "~/overcloudrc"
            dest: "{{ inventory_dir }}/keystonerc"
            flat: yes

- name: Set overcloud credentials
  hosts: controller[0]
  roles:
      - {role: installer/ospd/overcloud/credentials, overcloudrc: "{{ inventory_dir }}/keystonerc" }

- name: Run setup as root, to get all Ansible facts
  hosts: controller
  gather_facts: yes
  become: yes
  become_method: sudo
  tasks:
      - setup:

- name: Setup pacemaker for instance HA
  hosts: controller[0]
  gather_facts: yes
  become: yes
  environment:
      OS_USERNAME: "{{ oc_username.stdout | default('admin') }}"
      OS_PASSWORD: "{{ oc_password.stdout }}"
      OS_AUTH_URL: "{{ oc_auth.stdout }}"
      OS_TENANT_NAME: "{{ oc_tenant.stdout | default('admin') }}"
  tasks:
      - name: Create NovaEvacuate resource
        shell: "pcs resource create nova-evacuate ocf:openstack:NovaEvacuate auth_url=$OS_AUTH_URL username=$OS_USERNAME password=$OS_PASSWORD tenant_name=$OS_TENANT_NAME no_shared_storage=0"

      - name: Create pacemaker constraints to start VIP resources before nova-evacuate
        shell: |
            for i in $(pcs status | grep IP | awk '{ print $1 }')
              do pcs constraint order start $i then nova-evacuate
            done

      - name: Create pacemaker constraints to start openstack services before nova-evacuate
        shell: "pcs constraint order start {{ item }} then nova-evacuate require-all=false"
        with_items:
            - openstack-glance-api-clone
            - neutron-metadata-agent-clone
            - openstack-nova-conductor-clone
        when: "{{ installer.product.version | int }} < 10"

      - name: Disable keystone resource
        shell: "pcs resource disable openstack-keystone --wait=900"
        when: "{{ installer.product.version | int }} < 9"

      # Keystone resource was replaced by openstack-core resource in RHOS9
      - name: Disable openstack-core resource
        shell: "pcs resource disable openstack-core --wait=900"
        when: "{{ installer.product.version | int }} == 9"

      - name: Set controller pacemaker property on controllers
        shell: "pcs property set --node {{ hostvars[item]['ansible_hostname'] }} osprole=controller"
        with_items: "{{ groups['controller'] }}"

      - name: Get stonith devices
        shell: "pcs stonith | awk '{print $1}' | tr '\n' ' '"
        register: stonithdevs

      - name: Setup stonith devices
        shell: |
            for i in $(sudo cibadmin -Q --xpath //primitive --node-path | awk -F "id='" '{print $2}' | awk -F "'" '{print $1}' | uniq); do
              found=0
              if [ -n "{{ stonithdevs.stdout }}" ]; then
                for x in {{ stonithdevs.stdout }}; do
                  if [ "$x" == "$i" ]; then
                    found=1
                  fi
                done
              fi
              if [ $found = 0 ]; then
                sudo pcs constraint location $i rule resource-discovery=exclusive score=0 osprole eq controller
              fi
            done

      - name: Create compute pacemaker resources and constraints
        shell: |
            pcs resource create neutron-openvswitch-agent-compute systemd:neutron-openvswitch-agent --clone interleave=true --disabled --force
            pcs constraint location neutron-openvswitch-agent-compute-clone rule resource-discovery=exclusive score=0 osprole eq compute
            pcs resource create libvirtd-compute systemd:libvirtd --clone interleave=true --disabled --force
            pcs constraint location libvirtd-compute-clone rule resource-discovery=exclusive score=0 osprole eq compute
            pcs constraint order start neutron-openvswitch-agent-compute-clone then libvirtd-compute-clone
            pcs constraint colocation add libvirtd-compute-clone with neutron-openvswitch-agent-compute-clone
            pcs resource create ceilometer-compute systemd:openstack-ceilometer-compute --clone interleave=true --disabled --force
            pcs constraint location ceilometer-compute-clone rule resource-discovery=exclusive score=0 osprole eq compute
            pcs constraint order start libvirtd-compute-clone then ceilometer-compute-clone
            pcs constraint colocation add ceilometer-compute-clone with libvirtd-compute-clone

            pcs resource create nova-compute-checkevacuate ocf:openstack:nova-compute-wait auth_url=$OS_AUTH_URL username=$OS_USERNAME password=$OS_PASSWORD tenant_name=$OS_TENANT_NAME no_shared_storage=0 op start timeout=300 --clone interleave=true --disabled --force
            pcs constraint location nova-compute-checkevacuate-clone rule resource-discovery=exclusive score=0 osprole eq compute

            pcs resource create nova-compute systemd:openstack-nova-compute --clone interleave=true --disabled --force
            pcs constraint location nova-compute-clone rule resource-discovery=exclusive score=0 osprole eq compute
            pcs constraint order start nova-compute-checkevacuate-clone then nova-compute-clone require-all=true

            pcs constraint order start nova-compute-clone then nova-evacuate require-all=false
            pcs constraint order start libvirtd-compute-clone then nova-compute-clone
            pcs constraint colocation add nova-compute-clone with libvirtd-compute-clone

      - name: Create pacemaker constraint for neutron-server, nova-conductor and ceilometer-notification
        shell: |
            pcs constraint order start neutron-server-clone then neutron-openvswitch-agent-compute-clone require-all=false
            pcs constraint order start openstack-ceilometer-notification-clone then ceilometer-compute-clone require-all=false
            pcs constraint order start openstack-nova-conductor-clone then nova-compute-checkevacuate-clone require-all=false
        when: "{{ installer.product.version | int }} < 10"

      # TODO (mkrcmari) Create a play for baremetal computes fencing setup
      - name: Set up virt-fencing for compute nodes
        shell: |
            COMPUTENAME={{ hostvars[item]['ansible_hostname'] }}
            VM_UUID={{ hostvars[item]['ansible_product_uuid'] }}
            pcs stonith delete my-stonith-xvm-$COMPUTENAME || /bin/true
            pcs stonith create my-stonith-xvm-$COMPUTENAME fence_xvm port=$VM_UUID pcmk_host_list=$COMPUTENAME use_uuid=1 op monitor interval=30s
        with_items: "{{ groups['compute'] }}"
        when: "'virthost' in groups"

      - name: Create fence-nova pacemaker resource
        shell: "pcs stonith create fence-nova fence_compute auth-url=$OS_AUTH_URL login=$OS_USERNAME passwd=$OS_PASSWORD tenant-name=$OS_TENANT_NAME domain=localdomain record-only=1 no_shared_storage=0 action=off --force"

      - name: Set cluster recheck interval to 1 minute
        shell: "pcs property set cluster-recheck-interval=1min"

      - name: Create pacemaker remote resource on compute nodes
        shell: "pcs resource create {{ hostvars[item]['ansible_hostname'] }} ocf:pacemaker:remote reconnect_interval=240 op monitor interval=20"
        with_items: "{{ groups['compute'] }}"

      - name: Set osprole for compute nodes
        shell: "pcs property set --node {{ hostvars[item]['ansible_hostname'] }} osprole=compute"
        with_items: "{{ groups['compute'] }}"

      - name: Add pacemaker stonith devices of compute nodes to level 1
        shell: "pcs stonith level add 1 {{ hostvars[item]['ansible_hostname'] }} my-stonith-xvm-{{ hostvars[item]['ansible_hostname'] }},fence-nova"
        with_items: "{{ groups['compute'] }}"

      - name: Enable keystone resource
        shell: "pcs resource enable openstack-keystone"
        when: "{{ installer.product.version | int }} < 9"

      - name: Enable openstack-core resource
        shell: "pcs resource enable openstack-core"
        when: "{{ installer.product.version | int }} == 9"

      - name: Wait for httpd service to be started
        shell: "systemctl show httpd --property=ActiveState"
        register: httpd_status_result
        until: httpd_status_result.stdout.find('inactive') == -1 and httpd_status_result.stdout.find('activating') == -1
        retries: 30
        delay: 10
        when: "{{ installer.product.version | int }} > 8"

      - name: Enable compute resources
        shell: "pcs resource enable {{ item }}"
        with_items:
            - neutron-openvswitch-agent-compute
            - libvirtd-compute
            - ceilometer-compute
            - nova-compute-checkevacuate
            - nova-compute

- name: Wait for libvirt resource to be available on compute nodes
  hosts: compute
  tasks:
      - wait_for:
            port: 16509
            delay: 20

- name: Cleanup all pacemaker resource
  hosts: controller[0]
  become: yes
  tasks:
      - name: Cleanup all pacemaker resources
        shell: "pcs --force resource cleanup"
